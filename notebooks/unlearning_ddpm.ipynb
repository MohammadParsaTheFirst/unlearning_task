{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. Setup and Configuration\n",
    "This cell installs necessary libraries (uncomment to run), sets all the required path variables, and defines core hyperparameters like the number of training and unlearning epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 0: Imstalling the requirements\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of libraries to check and install\n",
    "libraries = [\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"diffusers\",\n",
    "    \"tqdm\",\n",
    "    \"typing\",\n",
    "    \"os\",\n",
    "    \"random\",\n",
    "    \"numpy\",\n",
    "    \"torchmetrics\"\n",
    "]\n",
    "\n",
    "def check_and_install(lib_name):\n",
    "    try:\n",
    "        importlib.import_module(lib_name)\n",
    "        print(f\"{lib_name} already exists.\")\n",
    "    except ImportError:\n",
    "        print(f\"{lib_name} not found. Installing...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib_name])\n",
    "            importlib.import_module(lib_name)\n",
    "            print(f\"{lib_name} was installed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install {lib_name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for lib in libraries:\n",
    "        check_and_install(lib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple, Dict\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import metric classes from torchmetrics\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.kid import KernelInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "\n",
    "# --- Install Required Libraries (Uncomment and run if needed) ---\n",
    "# !pip install torch torchvision diffusers tqdm torchmetrics numpy\n",
    "\n",
    "# --- Project Paths and Configuration ---\n",
    "\n",
    "# Define your project directory (where data and models will be saved)\n",
    "PROJECT_DIR = \".\" \n",
    "\n",
    "# Data paths (will be created/populated by preprocessing)\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"data\")\n",
    "FULL_DATA_PATH = os.path.join(DATA_DIR, \"full_dataset.pt\")\n",
    "RETAIN_PATH = os.path.join(DATA_DIR, \"retain_dataset.pt\")\n",
    "FORGET_PATH = os.path.join(DATA_DIR, \"forget_dataset.pt\") \n",
    "\n",
    "# Model save directories\n",
    "VANILLA_MODEL_DIR = os.path.join(PROJECT_DIR, \"Vanilla_DDPM\")\n",
    "UNLEARNED_SISS_DIR = os.path.join(PROJECT_DIR, \"Unlearned_SISS\")\n",
    "UNLEARNED_DYNAMIC_DIR = os.path.join(PROJECT_DIR, \"Unlearned_Dynamic\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(VANILLA_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(UNLEARNED_SISS_DIR, exist_ok=True)\n",
    "os.makedirs(UNLEARNED_DYNAMIC_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_EPOCHS = 10 \n",
    "UNLEARN_EPOCHS = 50 \n",
    "NUM_SAMPLES_EVAL = 1000 # Use 10,000 for production-grade evaluation\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. Core Functions and Utilities\n",
    "This cell defines all the necessary classes (`ClassEmbedder`, `CustomDataset`, `ReferenceDataset`) and the main step-wise functions (`define_model`, `preprocess_data_func`, `train_ddpm_func`, `unlearn_ddpm_func`, `sampling_func`, `evaluate_ddpm_func`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Core Functions and Utilities\n",
    "\n",
    "# --- Utility Classes ---\n",
    "\n",
    "class ClassEmbedder(nn.Module):\n",
    "    \"\"\"Class embedding module used in all scripts.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 10, embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "    def forward(self, labels: torch.Tensor) -> torch.Tensor:\n",
    "        # Returns (batch_size, embedding_dim)\n",
    "        return self.embedding_layer(labels)\n",
    "\n",
    "class LambdaNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A small network to dynamically predict the lambda weight for the dynamic unlearning method.\n",
    "    It takes the retain set's class embeddings as input.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus() # Ensures lambda is non-negative\n",
    "        )\n",
    "\n",
    "    def forward(self, embedding: torch.Tensor) -> torch.Tensor:\n",
    "        # embedding shape: (batch_size, 1, embedding_dim)\n",
    "        # Squeeze the middle dimension for the linear layer: (batch_size, embedding_dim)\n",
    "        output = self.net(embedding.squeeze(1))\n",
    "        # Return a single scalar (mean of the batch outputs) to serve as the lambda for the batch loss\n",
    "        return output.mean()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Custom dataset for MNIST-like data with optional transforms.\"\"\"\n",
    "\n",
    "    def __init__(self, images: torch.Tensor, labels: torch.Tensor, transform: Optional[transforms.Compose] = None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # x shape: (H, W) or (1, H, W)\n",
    "        x = self.images[idx].unsqueeze(0)  # Ensure shape (1, H, W)\n",
    "        y = self.labels[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "class ReferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to load the original reference data (full_dataset.pt) \n",
    "    for comparison in evaluation metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, pt_file_path: str, transform: transforms.Compose):\n",
    "        data = torch.load(pt_file_path)\n",
    "        self.images = data['images']\n",
    "        self.labels = data['labels']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        # Load and transform image\n",
    "        img = self.images[idx].unsqueeze(0)\n",
    "        return self.transform(img)\n",
    "\n",
    "# --- Metric Helper Function ---\n",
    "\n",
    "def calculate_psnr(img1: torch.Tensor, img2: torch.Tensor, data_range: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"Calculates PSNR between two tensors, assuming data in range [-1, 1].\"\"\"\n",
    "    mse = F.mse_loss(img1, img2)\n",
    "    if mse == 0:\n",
    "        return torch.tensor(float('inf'), device=img1.device)\n",
    "    return 10 * torch.log10(1.0 / mse)\n",
    "\n",
    "# --- Model Definition Function ---\n",
    "\n",
    "def define_model(load_path_unet: Optional[str] = None, load_path_embedder: Optional[str] = None) -> Tuple[UNet2DConditionModel, ClassEmbedder]:\n",
    "    \"\"\"Instantiates and optionally loads weights for the UNet and ClassEmbedder.\"\"\"\n",
    "    model = UNet2DConditionModel(\n",
    "        sample_size=IMG_SIZE,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        layers_per_block=2,\n",
    "        block_out_channels=(128, 256, 512),\n",
    "        down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "        up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
    "        cross_attention_dim=256,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    class_embedder = ClassEmbedder(num_classes=10, embedding_dim=256).to(DEVICE)\n",
    "\n",
    "    if load_path_unet and os.path.exists(load_path_unet):\n",
    "        print(f\"Loading UNet weights from {load_path_unet}\")\n",
    "        model.load_state_dict(torch.load(load_path_unet, map_location=DEVICE))\n",
    "\n",
    "    if load_path_embedder and os.path.exists(load_path_embedder):\n",
    "        print(f\"Loading ClassEmbedder weights from {load_path_embedder}\")\n",
    "        class_embedder.load_state_dict(torch.load(load_path_embedder, map_location=DEVICE))\n",
    "\n",
    "    return model, class_embedder\n",
    "\n",
    "\n",
    "# --- Step 1: Data Preprocessing ---\n",
    "\n",
    "def preprocess_data_func(dataset_dir: str, full_data_path: str, retain_path: str, forget_path: str):\n",
    "    \"\"\"\n",
    "    Implements the logic of preprocess_data.py to create the full, retain, and forget sets.\n",
    "    The forget set is defined as the FMNIST trousers (label 1 augmentation).\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Data Preprocessing ---\")\n",
    "    \n",
    "    # 1. Download and prepare datasets\n",
    "    transform_raw = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist = datasets.MNIST(root=dataset_dir, train=True, download=True, transform=transform_raw)\n",
    "    fmnist = datasets.FashionMNIST(root=dataset_dir, train=True, download=True, transform=transform_raw)\n",
    "    \n",
    "    # Separate MNIST label 1 indices and others\n",
    "    mnist_indices = list(range(len(mnist)))\n",
    "    mnist_label_1_indices = [i for i in mnist_indices if mnist[i][1] == 1]\n",
    "    mnist_other_indices = [i for i in mnist_indices if mnist[i][1] != 1]\n",
    "    \n",
    "    # FashionMNIST trousers (label 1)\n",
    "    fmnist_trouser_indices = [i for i, (_, label) in enumerate(fmnist) if label == 1]\n",
    "\n",
    "    # Sample an equal number of FMNIST trousers as MNIST label 1 images\n",
    "    num_label_1 = len(mnist_label_1_indices)\n",
    "    #fmnist_trouser_sample_indices = random.sample(fmnist_trouser_indices, num_label_1)\n",
    "    # Select 10% of MNIST label 1 count for trousers to add (at least one)\n",
    "    num_trousers_to_add = max(1, num_label_1 // 10)\n",
    "    fmnist_trouser_sample_indices = random.sample(fmnist_trouser_indices, num_trousers_to_add)\n",
    "\n",
    "    # 2. Build the Combined (Full) Dataset\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    is_forget_mask_list = []\n",
    "    \n",
    "    # A. Add all MNIST non-label-1 images as is (RETAIN)\n",
    "    for idx in mnist_other_indices:\n",
    "        img, label = mnist[idx]\n",
    "        all_images.append(img.squeeze())\n",
    "        all_labels.append(label)\n",
    "        is_forget_mask_list.append(False)\n",
    "        \n",
    "    # B. Add MNIST label 1 images (RETAIN)\n",
    "    for idx in mnist_label_1_indices:\n",
    "        img, _ = mnist[idx]\n",
    "        all_images.append(img.squeeze())\n",
    "        all_labels.append(1) # Label is 1\n",
    "        is_forget_mask_list.append(False)\n",
    "        \n",
    "    # C. Add FashionMNIST trousers with label 1 (FORGET - The augmentation)\n",
    "    for idx in fmnist_trouser_sample_indices:\n",
    "        img, _ = fmnist[idx]\n",
    "        all_images.append(img.squeeze())\n",
    "        all_labels.append(1)  # Label as 1\n",
    "        is_forget_mask_list.append(True)\n",
    "        \n",
    "    # Shuffle the entire combined dataset\n",
    "    combined = list(zip(all_images, all_labels, is_forget_mask_list))\n",
    "    random.shuffle(combined)\n",
    "    \n",
    "    all_images, all_labels, is_forget_mask_list = zip(*combined)\n",
    "    \n",
    "    full_images = torch.stack(all_images)\n",
    "    full_labels = torch.tensor(all_labels)\n",
    "    is_forget_mask = torch.tensor(is_forget_mask_list)\n",
    "\n",
    "    # 3. Define Retain and Forget Sets\n",
    "    # The forget set is only the FMNIST Trousers\n",
    "    forget_indices = torch.where(is_forget_mask)[0]\n",
    "    retain_indices = torch.where(~is_forget_mask)[0]\n",
    "    \n",
    "    print(f\"Total samples: {len(full_images)}\")\n",
    "    print(f\"Forget samples (FMNIST Trousers): {len(forget_indices)}\")\n",
    "    print(f\"Retain samples: {len(retain_indices)}\")\n",
    "    \n",
    "    # 4. Save Datasets\n",
    "    torch.save({'images': full_images, 'labels': full_labels}, full_data_path)\n",
    "    print(f\"Saved Full Dataset to {full_data_path}\")\n",
    "\n",
    "    torch.save({\n",
    "        'images': full_images[retain_indices], \n",
    "        'labels': full_labels[retain_indices]\n",
    "    }, retain_path)\n",
    "    print(f\"Saved Retain Dataset to {retain_path}\")\n",
    "\n",
    "    torch.save({\n",
    "        'images': full_images[forget_indices], \n",
    "        'labels': full_labels[forget_indices]\n",
    "    }, forget_path)\n",
    "    print(f\"Saved Forget Dataset to {forget_path}\")\n",
    "\n",
    "\n",
    "# --- Step 2: DDPM Training ---\n",
    "\n",
    "def train_ddpm_func(data_path: str, model_dir: str, epochs: int = TRAIN_EPOCHS):\n",
    "    \"\"\"Trains the Conditional DDPM model on the full dataset and saves the weights.\"\"\"\n",
    "    print(\"\\n--- Starting Vanilla DDPM Training ---\")\n",
    "    \n",
    "    # Load dataset\n",
    "    data = torch.load(data_path)\n",
    "    images = data[\"images\"]\n",
    "    labels = data[\"labels\"]\n",
    "\n",
    "    # Transform (Resize to IMG_SIZE=64, Normalize to [-1, 1])\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "        transforms.Lambda(lambda t: t.to(torch.float32))\n",
    "    ])\n",
    "\n",
    "    dataset = CustomDataset(images, labels, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Model and Embedder\n",
    "    model, class_embedder = define_model()\n",
    "\n",
    "    # Scheduler and optimizer\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\n",
    "    optimizer = torch.optim.AdamW(list(model.parameters()) + list(class_embedder.parameters()), lr=1e-4)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            noise = torch.randn_like(images)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (images.shape[0],), device=DEVICE).long()\n",
    "            noisy = noise_scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "            embeddings = class_embedder(labels).unsqueeze(1)\n",
    "            noise_pred = model(noisy, timesteps, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    # Save model\n",
    "    unet_path = os.path.join(model_dir, \"unet_final.pt\")\n",
    "    embed_path = os.path.join(model_dir, \"class_embedder.pt\")\n",
    "    torch.save(model.state_dict(), unet_path)\n",
    "    torch.save(class_embedder.state_dict(), embed_path)\n",
    "    print(f\"Vanilla model saved to {model_dir}\")\n",
    "\n",
    "\n",
    "# --- Step 3: Unlearning (SISS and Dynamic) ---\n",
    "\n",
    "def unlearn_ddpm_func(\n",
    "    vanilla_model_dir: str,\n",
    "    retain_path: str,\n",
    "    forget_path: str,\n",
    "    unlearn_model_dir: str,\n",
    "    method: str,\n",
    "    epochs: int = UNLEARN_EPOCHS\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs unlearning using the specified method (siss or dynamic) and saves the weights.\n",
    "    The 'dynamic' method uses a learnable LambdaNetwork.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting {method.upper()} Unlearning ---\")\n",
    "\n",
    "    # Load Vanilla Model\n",
    "    unet_path = os.path.join(vanilla_model_dir, \"unet_final.pt\")\n",
    "    embed_path = os.path.join(vanilla_model_dir, \"class_embedder.pt\")\n",
    "    model, class_embedder = define_model(unet_path, embed_path)\n",
    "\n",
    "    # Initialize LambdaNetwork for dynamic method\n",
    "    lambda_net = LambdaNetwork().to(DEVICE)\n",
    "    lambda_net_params = list(lambda_net.parameters()) if method.lower() == 'dynamic' else []\n",
    "\n",
    "    # Load Data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "        transforms.Lambda(lambda t: t.to(torch.float32))\n",
    "    ])\n",
    "\n",
    "    retain_data = torch.load(retain_path)\n",
    "    forget_data = torch.load(forget_path)\n",
    "\n",
    "    retain_dataset = CustomDataset(retain_data['images'], retain_data['labels'], transform)\n",
    "    forget_dataset = CustomDataset(forget_data['images'], forget_data['labels'], transform)\n",
    "\n",
    "    retain_dataloader = DataLoader(retain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    forget_dataloader = DataLoader(forget_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Scheduler and optimizer\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\n",
    "\n",
    "    # Include LambdaNetwork parameters only for the 'dynamic' method\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(model.parameters()) + list(class_embedder.parameters()) + lambda_net_params,\n",
    "        lr=1e-5\n",
    "    )\n",
    "\n",
    "    # Base lambda for SISS (fixed)\n",
    "    lambda_param_siss = 1.0\n",
    "\n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    if method.lower() == 'dynamic':\n",
    "        lambda_net.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        retain_iter = iter(retain_dataloader)\n",
    "        forget_iter = iter(forget_dataloader)\n",
    "\n",
    "        num_batches = max(len(retain_dataloader), len(forget_dataloader))\n",
    "        progress_bar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs} ({method.upper()})\")\n",
    "\n",
    "        for step in progress_bar:\n",
    "            # Cycle through dataloaders\n",
    "            try:\n",
    "                images_r, labels_r = next(retain_iter)\n",
    "            except StopIteration:\n",
    "                retain_iter = iter(retain_dataloader)\n",
    "                images_r, labels_r = next(retain_iter)\n",
    "\n",
    "            try:\n",
    "                images_f, labels_f = next(forget_iter)\n",
    "            except StopIteration:\n",
    "                forget_iter = iter(forget_dataloader)\n",
    "                images_f, labels_f = next(forget_iter)\n",
    "\n",
    "            images_r, labels_r = images_r.to(DEVICE), labels_r.to(DEVICE)\n",
    "            images_f, labels_f = images_f.to(DEVICE), labels_f.to(DEVICE)\n",
    "\n",
    "            # --- Retain & Forget Loss Calculation ---\n",
    "\n",
    "            # 1. Retain Loss (J_r)\n",
    "            noise_r = torch.randn_like(images_r)\n",
    "            timesteps_r = torch.randint(0, noise_scheduler.config.num_train_timesteps, (images_r.shape[0],), device=DEVICE).long()\n",
    "            noisy_r = noise_scheduler.add_noise(images_r, noise_r, timesteps_r)\n",
    "            embeddings_r = class_embedder(labels_r).unsqueeze(1)\n",
    "            noise_pred_r = model(noisy_r, timesteps_r, encoder_hidden_states=embeddings_r).sample\n",
    "            loss_r = F.mse_loss(noise_pred_r, noise_r)\n",
    "\n",
    "            # 2. Forget Loss (J_f)\n",
    "            noise_f = torch.randn_like(images_f)\n",
    "            timesteps_f = torch.randint(0, noise_scheduler.config.num_train_timesteps, (images_f.shape[0],), device=DEVICE).long()\n",
    "            noisy_f = noise_scheduler.add_noise(images_f, noise_f, timesteps_f)\n",
    "            embeddings_f = class_embedder(labels_f).unsqueeze(1)\n",
    "            noise_pred_f = model(noisy_f, timesteps_f, encoder_hidden_states=embeddings_f).sample\n",
    "            loss_f = F.mse_loss(noise_pred_f, noise_f)\n",
    "\n",
    "            # --- Weighted Loss Combination ---\n",
    "\n",
    "            if method.lower() == 'siss':\n",
    "                # SISS: J_total = lambda * J_r - 1.0 * J_f\n",
    "                lambda_effective = lambda_param_siss\n",
    "                total_loss = lambda_effective * loss_r - 1.0 * loss_f\n",
    "\n",
    "            elif method.lower() == 'dynamic':\n",
    "                # Dynamic: J_total = max(0, lambda_dynamic * J_r - 1.0 * J_f)\n",
    "                # Get the dynamic lambda from the network, conditioned on retain class embeddings\n",
    "                lambda_dynamic_tensor = lambda_net(embeddings_r)\n",
    "                lambda_effective = lambda_dynamic_tensor.item() # Use the scalar value for display\n",
    "\n",
    "                weighted_loss = lambda_dynamic_tensor * loss_r - 1.0 * loss_f\n",
    "                total_loss = torch.clamp(weighted_loss, min=0.0)\n",
    "\n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            progress_bar.set_postfix({'loss_total': total_loss.item(), 'loss_r': loss_r.item(), 'loss_f': loss_f.item(), 'lambda': f\"{lambda_effective:.3f}\"})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished. Avg Total Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "    # Save Unlearned model and LambdaNetwork (if dynamic)\n",
    "    unet_name = f\"unet_unlearned_{method}.pt\"\n",
    "    embed_name = f\"class_embedder_unlearned_{method}.pt\"\n",
    "\n",
    "    os.makedirs(unlearn_model_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(unlearn_model_dir, unet_name))\n",
    "    torch.save(class_embedder.state_dict(), os.path.join(unlearn_model_dir, embed_name))\n",
    "\n",
    "    if method.lower() == 'dynamic':\n",
    "        lambda_net_path = os.path.join(unlearn_model_dir, \"lambda_net.pt\")\n",
    "        torch.save(lambda_net.state_dict(), lambda_net_path)\n",
    "        print(f\"LambdaNetwork saved to {lambda_net_path}\")\n",
    "\n",
    "    print(f\"{method.upper()} unlearned model saved to {unlearn_model_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unlearn_ssis(\n",
    "    config, retain_loader, forget_loader, model, class_embedder, n, k, device\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform SISS unlearning: fixed lambda weighting between retain and forget losses.\n",
    "    \"\"\"\n",
    "    epochs = config.get(\"epochs\", 50)\n",
    "    lr = config.get(\"lr\", 1e-5)\n",
    "    lambda_fixed = config.get(\"lambda_\", 1.0)\n",
    "\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(model.parameters()) + list(class_embedder.parameters()),\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    retain_losses, forget_losses = [], []\n",
    "\n",
    "    model.train()\n",
    "    class_embedder.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        retain_iter = iter(retain_loader)\n",
    "        forget_iter = iter(forget_loader)\n",
    "        num_batches = max(len(retain_loader), len(forget_loader))\n",
    "\n",
    "        pbar = tqdm(range(num_batches), desc=f\"SISS Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for step in pbar:\n",
    "            try:\n",
    "                images_r, labels_r = next(retain_iter)\n",
    "            except StopIteration:\n",
    "                retain_iter = iter(retain_loader)\n",
    "                images_r, labels_r = next(retain_iter)\n",
    "\n",
    "            try:\n",
    "                images_f, labels_f = next(forget_iter)\n",
    "            except StopIteration:\n",
    "                forget_iter = iter(forget_loader)\n",
    "                images_f, labels_f = next(forget_iter)\n",
    "\n",
    "            images_r, labels_r = images_r.to(device), labels_r.to(device)\n",
    "            images_f, labels_f = images_f.to(device), labels_f.to(device)\n",
    "\n",
    "            # Retain loss\n",
    "            noise_r = torch.randn_like(images_r)\n",
    "            timesteps_r = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps,\n",
    "                (images_r.shape[0],), device=device\n",
    "            ).long()\n",
    "            noisy_r = noise_scheduler.add_noise(images_r, noise_r, timesteps_r)\n",
    "            emb_r = class_embedder(labels_r).unsqueeze(1)\n",
    "            noise_pred_r = model(noisy_r, timesteps_r, encoder_hidden_states=emb_r).sample\n",
    "            loss_r = F.mse_loss(noise_pred_r, noise_r)\n",
    "\n",
    "            # Forget loss\n",
    "            noise_f = torch.randn_like(images_f)\n",
    "            timesteps_f = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps,\n",
    "                (images_f.shape[0],), device=device\n",
    "            ).long()\n",
    "            noisy_f = noise_scheduler.add_noise(images_f, noise_f, timesteps_f)\n",
    "            emb_f = class_embedder(labels_f).unsqueeze(1)\n",
    "            noise_pred_f = model(noisy_f, timesteps_f, encoder_hidden_states=emb_f).sample\n",
    "            loss_f = F.mse_loss(noise_pred_f, noise_f)\n",
    "\n",
    "            # Total weighted loss\n",
    "            total_loss = lambda_fixed * loss_r - 1.0 * loss_f\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            retain_losses.append(loss_r.item())\n",
    "            forget_losses.append(loss_f.item())\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"L_total\": f\"{total_loss.item():.4f}\",\n",
    "                \"L_r\": f\"{loss_r.item():.4f}\",\n",
    "                \"L_f\": f\"{loss_f.item():.4f}\"\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Avg Total Loss: {np.mean(retain_losses[-num_batches:]):.4f}\")\n",
    "\n",
    "    return retain_losses, forget_losses\n"
    "\n",
    "\n",
    "\n",
    "# --- Step 4: Evaluation and Sampling Utilities ---\n",
    "\n",
    "def sampling_func(\n",
    "    model: UNet2DConditionModel, \n",
    "    class_embedder: ClassEmbedder, \n",
    "    condition_class: int, \n",
    "    num_samples: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Generates a batch of images conditioned on a class label.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\n",
    "    \n",
    "    sample = torch.randn(num_samples, 1, IMG_SIZE, IMG_SIZE, device=DEVICE)\n",
    "    labels = torch.full((num_samples,), condition_class, device=DEVICE).long()\n",
    "    embeddings = class_embedder(labels).unsqueeze(1)\n",
    "    \n",
    "    for t in tqdm(noise_scheduler.timesteps, desc=f\"Sampling Class {condition_class}\", leave=False):\n",
    "        with torch.no_grad():\n",
    "            model_output = model(sample, t, encoder_hidden_states=embeddings).sample\n",
    "            \n",
    "        sample = noise_scheduler.step(model_output, t, sample).prev_sample\n",
    "        \n",
    "    return sample\n",
    "\n",
    "\n",
    "def evaluate_ddpm_func(\n",
    "    full_data_path: str, \n",
    "    model_paths: Dict[str, str], \n",
    "    num_samples: int = NUM_SAMPLES_EVAL\n",
    "):\n",
    "    \"\"\"\n",
    "    Samples from the models, calculates evaluation metrics (FID, KID, IS, SSIM),\n",
    "    and prints the comparison summary.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Model Evaluation ---\")\n",
    "    all_results = {}\n",
    "    \n",
    "    # Transforms for Inception-based metrics (FID, KID, IS)\n",
    "    # Standard practice is to resize to 299x299 for InceptionNet\n",
    "    RESIZE_299 = transforms.Resize((299, 299), antialias=True) \n",
    "    RESIZE_64 = transforms.Resize((IMG_SIZE, IMG_SIZE), antialias=True)\n",
    "    \n",
    "    # 1. Prepare Reference Dataset\n",
    "    # Reference data is loaded and transformed to [-1, 1] (DDPM's range)\n",
    "    transform_ref = transforms.Compose([\n",
    "        RESIZE_64, \n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "        transforms.Lambda(lambda t: t.to(torch.float32)),\n",
    "    ])\n",
    "    ref_dataset = ReferenceDataset(full_data_path, transform_ref)\n",
    "    ref_dataloader = DataLoader(ref_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Function to transform DDPM output to metric input: [-1, 1] -> [0, 255] (uint8, 3-channel)\n",
    "    def preprocess_for_metrics(images: torch.Tensor) -> torch.Tensor:\n",
    "        # 1. Denormalize: [-1, 1] -> [0, 1]\n",
    "        images_01 = (images + 1) / 2\n",
    "        # 2. Resize to 299x299\n",
    "        images_299 = RESIZE_299(images_01)\n",
    "        # 3. Scale to [0, 255] and convert to uint8\n",
    "        images_uint8 = (images_299 * 255).to(torch.uint8)\n",
    "        # 4. InceptionNet expects 3-channel images (RGB), so we replicate the single channel\n",
    "        return images_uint8.repeat(1, 3, 1, 1)\n",
    "\n",
    "    # --- Prepare Metrics ---\n",
    "    # Using feature=64 for a lightweight/faster feature extractor\n",
    "    kid_metric = KernelInceptionDistance(subset_size=50, normalize=True, feature=64).to(DEVICE)\n",
    "    \n",
    "    for model_name, model_dir in model_paths.items():\n",
    "        print(f\"\\nEvaluating Model: {model_name}\")\n",
    "        kid_metric.reset()\n",
    "        \n",
    "        # Load model and embedder\n",
    "        if model_name == \"Vanilla\":\n",
    "            unet_path = os.path.join(model_dir, \"unet_final.pt\")\n",
    "            embed_path = os.path.join(model_dir, \"class_embedder.pt\")\n",
    "        else: # SISS or Dynamic\n",
    "            unet_path = os.path.join(model_dir, f\"unet_unlearned_{model_name.lower()}.pt\")\n",
    "            embed_path = os.path.join(model_dir, f\"class_embedder_unlearned_{model_name.lower()}.pt\")\n",
    "        \n",
    "        if not os.path.exists(unet_path):\n",
    "            print(f\"Skipping {model_name}: UNet weights not found at {unet_path}\")\n",
    "            continue\n",
    "            \n",
    "        model, class_embedder = define_model(unet_path, embed_path)\n",
    "        \n",
    "        # 2. Feed Reference Data to Metric\n",
    "        print(\"Processing Reference Data...\")\n",
    "        for ref_images in tqdm(ref_dataloader, desc=\"Ref Data\"):\n",
    "            ref_images = ref_images.to(DEVICE)\n",
    "            kid_metric.update(preprocess_for_metrics(ref_images), real=True)\n",
    "            \n",
    "        # 3. Sample from Model and Feed to Metric\n",
    "        print(\"Generating and Processing Samples...\")\n",
    "        \n",
    "        # Sample N/10 for each of the 10 classes\n",
    "        num_per_class = num_samples // 10\n",
    "        sampled_images_all = []\n",
    "        \n",
    "        for condition_class in range(10):\n",
    "            num_batches_sample = num_per_class // BATCH_SIZE\n",
    "            remainder = num_per_class % BATCH_SIZE\n",
    "            \n",
    "            for _ in range(num_batches_sample):\n",
    "                samples = sampling_func(model, class_embedder, condition_class, BATCH_SIZE)\n",
    "                kid_metric.update(preprocess_for_metrics(samples), real=False)\n",
    "                sampled_images_all.append(samples)\n",
    "            \n",
    "            if remainder > 0:\n",
    "                samples = sampling_func(model, class_embedder, condition_class, remainder)\n",
    "                kid_metric.update(preprocess_for_metrics(samples), real=False)\n",
    "                sampled_images_all.append(samples)\n",
    "        \n",
    "        # --- 4. Calculate Final Metrics ---\n",
    "        \n",
    "        sampled_images_all = torch.cat(sampled_images_all, dim=0).cpu() # [-1, 1]\n",
    "        \n",
    "        # FID and KID\n",
    "        try:\n",
    "            kid_mean, kid_std = kid_metric.compute()\n",
    "            fid_score = kid_metric.compute_fid()\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing FID/KID. Error: {e}\")\n",
    "            all_results[model_name] = {'FID': np.nan, 'KID_Mean': np.nan, 'KID_Std': np.nan, 'IS_Mean': np.nan, 'IS_Std': np.nan, 'SSIM': np.nan}\n",
    "            continue\n",
    "\n",
    "        # Inception Score (IS)\n",
    "        is_metric = InceptionScore(normalize=True, feature=64, splits=10).to(DEVICE)\n",
    "        is_metric.update(preprocess_for_metrics(sampled_images_all.to(DEVICE)), real=False)\n",
    "        is_mean, is_std = is_metric.compute()\n",
    "        \n",
    "        # Structural Similarity Index Measure (SSIM)\n",
    "        ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, kernel_size=3).to(DEVICE)\n",
    "        \n",
    "        # Compare a subset of generated images against a subset of real images (both [-1, 1] -> [0, 1])\n",
    "        num_ssim_samples = min(len(sampled_images_all), len(ref_dataset))\n",
    "        \n",
    "        # Generated images (rescaled to [0, 1])\n",
    "        gen_for_ssim = (sampled_images_all[:num_ssim_samples] + 1) / 2\n",
    "        \n",
    "        # Real images (loaded from dataset, denormalized to [0, 1])\n",
    "        real_images_tensor = torch.stack([ref_dataset[i][0] for i in range(num_ssim_samples)]).cpu()\n",
    "        real_for_ssim = (real_images_tensor + 1) / 2 \n",
    "\n",
    "        ssim_score = ssim_metric(gen_for_ssim.to(DEVICE), real_for_ssim.to(DEVICE))\n",
    "\n",
    "        # 5. Store Results\n",
    "        results = {\n",
    "            'FID': fid_score.item(),\n",
    "            'KID_Mean': kid_mean.item(),\n",
    "            'KID_Std': kid_std.item(),\n",
    "            'IS_Mean': is_mean.item(),\n",
    "            'IS_Std': is_std.item(),\n",
    "            'SSIM': ssim_score.item(),\n",
    "        }\n",
    "        all_results[model_name] = results\n",
    "        \n",
    "        # 6. Print Results Summary\n",
    "        print(\"\\n--- Evaluation Summary for \" + model_name + \" ---\")\n",
    "        print(f\"  FID: {results['FID']:.4f}\")\n",
    "        print(f\"  KID: {results['KID_Mean']:.4f} ± {results['KID_Std']:.4f}\")\n",
    "        print(f\"  IS: {results['IS_Mean']:.4f} ± {results['IS_Std']:.4f}\")\n",
    "        print(f\"  SSIM: {results['SSIM']:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # 7. Final Comparison Table\n",
    "    print(\"\\n\\n--- FINAL COMPARISON ---\")\n",
    "    header = \"Model Name\".ljust(15) + \" | \" + \"FID\".ljust(8) + \" | \" + \"KID (±)\".ljust(\n",
    "        12) + \" | \" + \"IS (±)\".ljust(12) + \" | \" + \"SSIM\".ljust(8)\n",
    "    print(header)\n",
    "    print(\"=\" * len(header))\n",
    "    \n",
    "    for name, res in all_results.items():\n",
    "        kid_str = f\"{res['KID_Mean']:.3f} ± {res['KID_Std']:.3f}\" if not np.isnan(res['KID_Mean']) else \"N/A\"\n",
    "        is_str = f\"{res['IS_Mean']:.3f} ± {res['IS_Std']:.3f}\" if not np.isnan(res['IS_Mean']) else \"N/A\"\n",
    "        fid_str = f\"{res['FID']:.4f}\" if not np.isnan(res['FID']) else \"N/A\"\n",
    "        ssim_str = f\"{res['SSIM']:.4f}\" if not np.isnan(res['SSIM']) else \"N/A\"\n",
    "        \n",
    "        line = name.ljust(15) + \" | \" + fid_str.ljust(8) + \" | \" + kid_str.ljust(\n",
    "            12) + \" | \" + is_str.ljust(12) + \" | \" + ssim_str.ljust(8)\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. Execute Data Preprocessing\n",
    "This cell executes the data preparation, creating the Full, Retain, and Forget datasets as `.pt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Execute Data Preprocessing\n",
    "\n",
    "# Creates full_dataset.pt, retain_dataset.pt, and forget_dataset.pt\n",
    "preprocess_data_func(DATA_DIR, FULL_DATA_PATH, RETAIN_PATH, FORGET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. Execute Vanilla Conditional DDPM Training\n",
    "This cell trains the initial conditional diffusion model on the full augmented dataset and saves the weights to the `Vanilla_DDPM` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Execute Vanilla Conditional DDPM Training\n",
    "\n",
    "train_ddpm_func(\n",
    "    data_path=FULL_DATA_PATH, \n",
    "    model_dir=VANILLA_MODEL_DIR, \n",
    "    epochs=TRAIN_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 5. Execute Unlearning (SISS and Dynamic)\n",
    "This cell loads the vanilla model and performs the two unlearning operations, saving the resulting models to their respective directories (`Unlearned_SISS`, `Unlearned_Dynamic`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Execute Unlearning (SISS and Dynamic)\n",
    "\n",
    "# SISS Unlearning\n",
    "unlearn_ddpm_func(\n",
    "    vanilla_model_dir=VANILLA_MODEL_DIR, \n",
    "    retain_path=RETAIN_PATH, \n",
    "    forget_path=FORGET_PATH, \n",
    "    unlearn_model_dir=UNLEARNED_SISS_DIR, \n",
    "    method='siss', \n",
    "    epochs=UNLEARN_EPOCHS\n",

    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dynamic Unlearning\n",
    "unlearn_ddpm_func(\n",
    "    vanilla_model_dir=VANILLA_MODEL_DIR, \n",
    "    retain_path=RETAIN_PATH, \n",
    "    forget_path=FORGET_PATH, \n",
    "    unlearn_model_dir=UNLEARNED_DYNAMIC_DIR, \n",
    "    method='dynamic', \n",
    "    epochs=UNLEARN_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 6. Execute Evaluation\n",
    "This final cell samples images from all three models (`Vanilla`, `SISS`, `Dynamic`) and calculates the evaluation scores (`FID`, `KID`, `IS`, `SSIM`) against the full reference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Execute Evaluation\n",
    "\n",
    "model_paths = {\n",
    "    \"Vanilla\": VANILLA_MODEL_DIR,\n",
    "    \"SISS\": UNLEARNED_SISS_DIR,\n",
    "    \"Dynamic\": UNLEARNED_DYNAMIC_DIR,\n",
    "}# make sure the models are in their corresponding dirs\n",
    "\n",
    "evaluate_ddpm_func(\n",
    "    full_data_path=FULL_DATA_PATH, \n",
    "    model_paths=model_paths, \n",
    "    num_samples=NUM_SAMPLES_EVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
